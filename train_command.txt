# train command
    # model_name, image_size, patch_size, pre_train, pre_ssl_train, augmentation, epochs, batch_size, weight_decay,
    # clip_grad, weight_decay_end, warmup_epochs, resume, pre_ssl_train_path


## self supervised train command [mae_vit_base_patch16]
python train.py --train_mode=self_supervised --augmentation=mae_ssl_train --batch_size=64 --epochs=600 --model_name=mae_vit_base_patch16 --image_size=224 --patch_size=16 --mask_ratio=0.75 --warmup_epochs=600

### MAE finetune command [vit_base_patch16]
### different image sizes
python train.py --train_mode=fine_tune --augmentation=mae_fine_tune_train --batch_size=32 --model_name=vit_base_patch16 --image_size=224 --patch_size=16 --epochs=60
python train.py --train_mode=fine_tune --augmentation=mae_fine_tune_train --batch_size=32 --model_name=vit_base_patch16 --image_size=326 --patch_size=16 --epochs=60
python train.py --train_mode=fine_tune --augmentation=mae_fine_tune_train --batch_size=32 --model_name=vit_base_patch16 --image_size=512 --patch_size=16 --epochs=60

### MAE ensemble post cross attention model finetune command [vit_base_patch16]
python train.py --train_mode=fine_tune --augmentation=mae_fine_tune_train --batch_size=32 --model_name=vit_base_patch16_ensemble_post_cross_attention --image_size=224 --patch_size=16 --epochs=40 --ensemble --cross_attention --pre_ssl_train_path
# using label smoothingï¼Œmixup and model_ema for training
python train.py --train_mode=fine_tune --augmentation=mae_fine_tune_train --batch_size=32 --model_name=vit_base_patch16_ensemble_post_cross_attention --image_size=224 --patch_size=16 --epochs=40 --ensemble --cross_attention --smoothing=0.1 --use_model_ema --mixup=0.8 --cutmix=1.0





